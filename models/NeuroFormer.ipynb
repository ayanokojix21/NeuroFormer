{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NeuroFormer\n",
        "\n",
        "**NeuroFormer** is a **modular Transformer architecture** built from scratch in PyTorch that supports:\n",
        "\n",
        "- **Decoder-only** — for language modeling and chatbots  \n",
        "- **Encoder-only** — for tasks like question answering  \n",
        "- **Encoder-Decoder** — for machine translation"
      ],
      "metadata": {
        "id": "vBwzxTt2GOt9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HImeo6zheLj7"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting Device to GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CbOMZWYTHbKy",
        "outputId": "3e317c32-cfe3-44d7-9678-df2a6b992693"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Utility Functions"
      ],
      "metadata": {
        "id": "18s1thF1IOtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pad Masking\n",
        "\n",
        "- Creates a padding mask to ignore pad tokens during attention."
      ],
      "metadata": {
        "id": "4QzjDz7b0H08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_mask(seq, pad_token=0):\n",
        "\n",
        "  # seq -> [batch_size, sequence_length]\n",
        "  mask = (seq != pad_token).unsqueeze(1).unsqueeze(1) # [B, 1, 1, T]\n",
        "  return mask"
      ],
      "metadata": {
        "id": "7efaMIMw0R2w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Casual Masking\n",
        "\n",
        "- Prevents a token from attending to future tokens during training, used in **Decoder** blocks.\n",
        "\n",
        "- Masking is applied in decoder block to enable **auto-regressive** generation"
      ],
      "metadata": {
        "id": "u8Q_mglTAEwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_mask(sequence_length, device):\n",
        "\n",
        "  # batch_size is referred as B and sequnece_length is referred as T while mentioning Dimentions\n",
        "\n",
        "  # Creating a Lower Triangular matrix\n",
        "  mask = torch.ones(sequence_length, sequence_length, device=device, dtype=torch.bool)\n",
        "  mask = torch.tril(mask) # Dimentions: (T, T)\n",
        "\n",
        "  # Adding batch_size, and num_heads\n",
        "  mask = mask.unsqueeze(0).unsqueeze(0) # Dimentions: (1, 1, T, T)\n",
        "  return mask"
      ],
      "metadata": {
        "id": "bKzpQLTbAZZc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Masking\n",
        "\n",
        "- It is a binary attention mask used in Cross Attention which are used in encoder-decoder architecture like T5\n",
        "- Prevent the **decoder** from attending to **pad** tokens in the encoder output."
      ],
      "metadata": {
        "id": "IAV086VCMS20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_mask(src_seq, tgt_seq, src_pad_token=0, tgt_pad_token=0):\n",
        "\n",
        "  # src_seq: [batch_size, src_len]\n",
        "  # tgt_seq: [batch_size, tgt_len]\n",
        "\n",
        "  batch_size, tgt_len = tgt_seq.shape\n",
        "  batch_size, src_len = src_seq.shape\n",
        "\n",
        "  # Create source padding mask: [batch_size, src_len]\n",
        "  src_valid = (src_seq != src_pad_token)  # True for valid tokens\n",
        "\n",
        "  # Create target padding mask: [batch_size, tgt_len]\n",
        "  tgt_valid = (tgt_seq != tgt_pad_token)  # True for valid tokens\n",
        "\n",
        "  # Each target position can attend to all valid source positions\n",
        "  cross_mask = src_valid.unsqueeze(1).expand(-1, tgt_len, -1)  # [B, tgt_len, src_len]\n",
        "\n",
        "  # Mask out padded target positions (they shouldn't attend to anything)\n",
        "  tgt_mask = tgt_valid.unsqueeze(-1)  # [batch_size, tgt_len, 1]\n",
        "  mask = cross_mask & tgt_mask  # [batch_size, tgt_len, src_len]\n",
        "\n",
        "  # Add num_heads: [batch_size, 1, tgt_len, src_len]\n",
        "  return mask.unsqueeze(1)"
      ],
      "metadata": {
        "id": "qIzYUZXmMsHd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_masks(*masks): # Takes Variable mask input\n",
        "\n",
        "  # If no Mask is passed retruns None\n",
        "  if not masks:\n",
        "      return None\n",
        "\n",
        "  # Take the first mask as starting point\n",
        "  combined = masks[0]\n",
        "\n",
        "  # Iterate over rest of masks and if mask is not None, it combines and return True only if all mask agrees on that position\n",
        "  for mask in masks[1:]:\n",
        "      if mask is not None:\n",
        "          combined = combined & mask\n",
        "\n",
        "  return combined"
      ],
      "metadata": {
        "id": "AdQepQlL5OIp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaled Dot Product Attention\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
        "$$\n",
        "\n",
        "- $Q$ -> It represents the query vector, or what the model is searching for\n",
        "- $K$ -> It represents the key vector, or all the words in the input sequence and is used to compare against query\n",
        "- $V$ -> It represents the value vector, the actual meaning of each word in the sentence.\n",
        "- $d_k$ -> Dimetionality of the Query/Key vector used for Scaling\n",
        "- $QK^T$ -> To find to similarity between the query vector and key vector\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yHM8P1T3IU-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask = None):\n",
        "  d_k = q.size()[-1] # head_dim\n",
        "  dot_product = torch.matmul(q, k.transpose(-1, -2)) # q: [B, num_heads, T, head_dim], kᵀ: [B, num_heads, head_dim, T], q @ kᵀ: [B, num_heads, T, T]\n",
        "  scaled = dot_product / math.sqrt(d_k) # [B, num_heads, T, T]\n",
        "\n",
        "  if mask is not None:\n",
        "    # Convert boolean mask to additive mask (-inf for False positions)\n",
        "    scaled = scaled.masked_fill(~mask , float('-inf')) # After Broadcasting: [B, num_heads, T, T]\n",
        "\n",
        "  scaled = scaled - scaled.max(dim=-1, keepdim=True)[0]  # subtract max before softmax [B, num_heads, T, T]\n",
        "  attention = F.softmax(scaled, dim=-1) # attention: [B, num_heads, T, T]\n",
        "  values = torch.matmul(attention, v) # v: [B, num_heads, T, head_dim] , Values: [B, num_heads, T, head_dim]\n",
        "\n",
        "  return attention, values"
      ],
      "metadata": {
        "id": "ybF88aqIIIZd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention\n",
        "\n",
        "$$MultiHead(Q, K, V) = Concat(head₁, ..., headₕ) · Wᵒ$$\n",
        "$$headᵢ = Attention(Q · Wᵢ^Q, K · Wᵢ^K, V · Wᵢ^V)$$\n",
        "\n",
        "### Multi-Head Attention splits the model into multiple **heads** that learns different types of relationship between the data.\n",
        "\n",
        "### Each individual head performs seperate scaled dot product attention independently and they improve the training speed through Parallelism.\n"
      ],
      "metadata": {
        "id": "DkQdWvTjNxSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module): # nn.Module so that it can inherit nn.Module functions and Becomes reusable and modular block\n",
        "\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model # Dimentions of Input Embeddings\n",
        "    self.num_heads = num_heads # Number of heads in the Multi-Head Attention\n",
        "    self.head_dim = d_model // num_heads # Dimentions of Embedding passed in each Head\n",
        "    self.qkv_layer = nn.Linear(d_model, 3 * d_model) # Projects the input as Query, Key and Value\n",
        "    self.linear_layer = nn.Linear(d_model, d_model) # After combining all the heads this layer concatenates result back to input embedding dimentions\n",
        "\n",
        "  def forward(self, x, mask = None):\n",
        "    batch_size, sequence_length, d_model = x.size() # Input X is of dimentions: [B, T, d_model]\n",
        "    qkv = self.qkv_layer(x) # It makes the dimentions: [B, T, 3 * d_model]\n",
        "\n",
        "    qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim) # 3 * d_model is converted to (num_heads, 3 * head_dim) as to process in each seperate head num_heads is included and 3 * head_dim as it contains q,k,v combined\n",
        "    qkv = qkv.permute(0, 2, 1, 3) # Reshaping it to [B, num_heads, T, 3 * head_dim] for efficient attention computation\n",
        "\n",
        "    q, k, v = qkv.chunk(3, dim=-1) # Splitting the last dimention for q,k,v and making it to [B, num_heads, T, head_dim]\n",
        "    attention, values = scaled_dot_product_attention(q, k, v, mask) # Calculating the attention and values\n",
        "\n",
        "    values = values.transpose(1, 2).contiguous() # Combines the output of all heads\n",
        "    values = values.reshape(batch_size, sequence_length, d_model) # dimentions: [B, T, d_model]\n",
        "    out = self.linear_layer(values) # Final transformation to combine the output of all heads into d_model: [B, T, d_model]\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "vGd8BM9hN25d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Normalization\n",
        "$$\n",
        "\\text{LayerNorm}(x) = \\gamma \\cdot \\left( \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} \\right) + \\beta\n",
        "$$\n",
        "Layer Normalization is a technique used to **stabilize** and **accelerate training** by normalizing the inputs across the features of each sample.\n",
        "**Applies per sample** across **features** (unlike BatchNorm which operates across the batch)."
      ],
      "metadata": {
        "id": "LgEXKj7bftQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.d_model = d_model\n",
        "    self.gamma = nn.Parameter(torch.ones(d_model)) # Learnable Parameter\n",
        "    self.beta = nn.Parameter(torch.zeros(d_model)) # Learnable Parameter\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    mean = x.mean(dim=-1, keepdim=True) # Mean\n",
        "    var = ((x - mean) ** 2).mean(dim=-1, keepdim=True) # Variance\n",
        "    std = (var + self.eps).sqrt() # Standard Deviation : eps is added to avoid division by 0\n",
        "\n",
        "    y = (x - mean) / std # Output\n",
        "    out = self.gamma * y + self.beta # Applying Learnable Parameters\n",
        "    return out"
      ],
      "metadata": {
        "id": "rATxaaTngG0k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Position-wise Feed Forward\n",
        "$$FFN(x) = max(0, x · W₁ + b₁) · W₂ + b₂$$\n",
        "- In Transformers, each token’s representation is passed through a feed-forward neural network independently and identically.\n",
        "\n",
        "- This is known as a Positionwise Feed-Forward Network (FFN) because it operates separately on each position (token) in the sequence."
      ],
      "metadata": {
        "id": "OPi29RGClVJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(d_model, hidden) # Hidden Layer converts dim from d_model to hidden\n",
        "    self.linear2 = nn.Linear(hidden, d_model) # Hidden Layer converts dim from hidden to d_model\n",
        "    self.relu = nn.ReLU() # ReLU Activation Function\n",
        "    self.dropout = nn.Dropout(p=drop_prob) # Dropout Layer\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "zoUd4rDTnurn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Cross Attention\n",
        "- Cross-attention is a key component of the encoder-decoder architecture used in models like T5 and Transformer for machine translation.\n",
        "\n",
        "- Query -> Decoder Input\n",
        "- Key -> Encoder Output\n",
        "- Value -> Encoder Output"
      ],
      "metadata": {
        "id": "6XxpX2MHoj5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadCrossAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model # Dimentions of Input Embedding\n",
        "    self.num_heads = num_heads # Number of Heads in Multi-Head Cross Attention\n",
        "    self.head_dim = d_model // num_heads # Dimention of Embedding Passed in each head\n",
        "    self.kv_layer = nn.Linear(d_model, 2 * d_model) # Projects the Input Embedding as Key, Value from Encoder Output\n",
        "    self.q_layer = nn.Linear(d_model, d_model) # Projects the Output Embedding as Query from Decoder Input\n",
        "    self.linear_layer = nn.Linear(d_model, d_model) # Concatenates the results of all heads and returning back the result to d_model dimentions\n",
        "\n",
        "  def forward(self, x, y, cross_mask = None):\n",
        "    batch_size, tgt_length, d_model = y.size() # Decoder Input Y of dimentions: [B, T_tgt, d_model]\n",
        "    src_length = x.size(1) # Encoder Output X\n",
        "\n",
        "    kv = self.kv_layer(x) # It makes dimentions: [B, T_src, 2 * d_model] i.e Key and Value Combined from Encoder Output\n",
        "    q = self.q_layer(y) # It makes dimentions: [B, T_tgt, d_model] i.e Query from Decoder Input\n",
        "\n",
        "    kv = kv.reshape(batch_size, src_length, self.num_heads, 2 * self.head_dim) # 2 * d_model is converted to num_heads as to process in seperate heads and 2 * head_dim i.e combined dimentions of key and value vector\n",
        "    q = q.reshape(batch_size, tgt_length, self.num_heads, self.head_dim) # d_model is converted to num_heads as to process in seperate heads and head_dim that represents the query vector dimentions\n",
        "\n",
        "    kv = kv.permute(0, 2, 1, 3)  # For efficient computation, new dimentions: [B, num_heads, T_src, 2 * head_dim]\n",
        "    q = q.permute(0, 2, 1, 3) # For efficient computation, new dimentions: [B, num_heads, T_tgt, head_dim]\n",
        "\n",
        "    k, v = kv.chunk(2, dim=-1) # Splitting the last dimention of kv and making it's dimention as [B, num_heads, T_src, head_dim]\n",
        "    attention, values = scaled_dot_product_attention(q, k, v, cross_mask) # Getting the Attention and values vector\n",
        "\n",
        "    values = values.transpose(1, 2).contiguous() # Combining back all the heads\n",
        "    values = values.reshape(batch_size, tgt_length, d_model) # Dimentions: [B, T_tgt, d_model]\n",
        "    out = self.linear_layer(values) # Learnable Parameter and converts back to original shape\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "5QhDsMB2pMZw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding\n",
        "\n",
        "- The input in attention mechanism is passed all at once and is generally **non auto-regressive** in nature.\n",
        "- The positionl Encoding help the sentence to get the idea of order otherwise it will treat \"I Love You\" and \"You Love I\" as **same**."
      ],
      "metadata": {
        "id": "v21osF3GJVXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, sequence_length, d_model, drop_prob=0.1):\n",
        "    super().__init__()\n",
        "    self.pos_embedding = nn.Embedding(sequence_length, d_model) # [T, d_model]\n",
        "    self.dropout = nn.Dropout(p = drop_prob)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    batch_size, sequence_length, d_model = x.size() # [B, T, d_model]\n",
        "    positions = torch.arange(sequence_length, device=x.device) # [T]\n",
        "    positions = positions.unsqueeze(0) # [1, T]\n",
        "    pos_emb = self.pos_embedding(positions) # [1, T, d_model]\n",
        "\n",
        "    x = x + pos_emb # [B, T, d_model] Broadcast Addition\n",
        "    x = self.dropout(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "0osk4w-kJzZj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Blocks: Encoder and Decoder Blocks"
      ],
      "metadata": {
        "id": "a5JSpLnQtYWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, hidden, drop_prob):\n",
        "    super().__init__()\n",
        "\n",
        "    # SubLayer 1 (MultiHeadAttention + LayerNorm)\n",
        "    self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "    self.norm1 = LayerNormalization(d_model)\n",
        "    self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    # SubLayer 2 (FeedForwardNetwork + LayerNorm)\n",
        "    self.ffn = PositionWiseFeedForward(d_model, hidden, drop_prob)\n",
        "    self.norm2 = LayerNormalization(d_model)\n",
        "    self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self, x, pad_mask = None):\n",
        "\n",
        "    # Self-Attention with residual network\n",
        "    attn_out = self.self_attention(x, pad_mask)\n",
        "    x = self.norm1(x + self.dropout1(attn_out))\n",
        "\n",
        "    # Feed Forward Network with residual network\n",
        "    ffn_out = self.ffn(x)\n",
        "    x = self.norm2(x + self.dropout2(ffn_out))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "7C7QYi8ctj5C"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, hidden, drop_prob):\n",
        "    super().__init__()\n",
        "\n",
        "    # SubLayer 1 (MultiHeadAttention + LayerNorm)\n",
        "    self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "    self.norm1 = LayerNormalization(d_model)\n",
        "    self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    # SubLayer2 (MultiHeadCrossAttention + LayerNorm)\n",
        "    self.cross_attention = MultiHeadCrossAttention(d_model, num_heads)\n",
        "    self.norm2 = LayerNormalization(d_model)\n",
        "    self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    # SubLayer 3 (FeedForwardNetwork + LayerNorm)\n",
        "    self.ffn = PositionWiseFeedForward(d_model, hidden, drop_prob)\n",
        "    self.norm3 = LayerNormalization(d_model)\n",
        "    self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self, y, encoder_output=None, self_mask=None, cross_mask=None):\n",
        "\n",
        "    # x: Encoder Output\n",
        "    # y: Decoder Input\n",
        "\n",
        "    # Masked-Self Attention with residual connections\n",
        "    attn_out = self.self_attention(y, self_mask)\n",
        "    y = self.norm1(y + self.dropout1(attn_out))\n",
        "\n",
        "    # Cross Attention (only if Encoder output is provided i.e x)\n",
        "    if encoder_output is not None:\n",
        "      cross_attn_out = self.cross_attention(encoder_output, y, cross_mask)\n",
        "      y = self.norm2(y + self.dropout2(cross_attn_out))\n",
        "\n",
        "    # Feed Forward Network with residual connections\n",
        "    ffn_out = self.ffn(y)\n",
        "    y = self.norm3(y + self.dropout3(ffn_out))\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "PSNuyMCewkwk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models: Encoder, Decoder, Encoder-Decoder"
      ],
      "metadata": {
        "id": "lnS5HTJ0nm3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, sequence_length, d_model, num_heads, hidden, num_layers, drop_prob=0.1, pad_token=0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.pad_token = pad_token\n",
        "    self.d_model = d_model\n",
        "\n",
        "    # Embeddings\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model) # Input Embedding\n",
        "    self.pos_enc = PositionalEncoding(sequence_length, d_model, drop_prob) # Positional Encoding\n",
        "\n",
        "    self.layers = nn.ModuleList([\n",
        "        EncoderBlock(d_model, num_heads, hidden, drop_prob)\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "\n",
        "    self.qa_head = nn.Linear(d_model, 2) # For Logits Calculation\n",
        "\n",
        "  def forward(self, input_ids, return_embeddings=False):\n",
        "\n",
        "    # Embedding + Positional Encoding\n",
        "    x = self.embedding(input_ids) * math.sqrt(self.d_model) # sqrt(d_model) is multiplied to Stabilize Variance as nn.Embedding generate number between 0 and 1\n",
        "    x = self.pos_enc(x) # Applying Positional Encodings\n",
        "\n",
        "    # Creating padding mask\n",
        "    pad_mask_tensor = pad_mask(input_ids, self.pad_token)\n",
        "\n",
        "    # Passing Through Multiple Encoder Layers\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, pad_mask_tensor)\n",
        "\n",
        "    if return_embeddings:\n",
        "      return x\n",
        "    else:\n",
        "      logits = self.qa_head(x)\n",
        "      start_logits = logits[:, :, 0]  # [B, T]\n",
        "      end_logits = logits[:, :, 1]    # [B, T]\n",
        "      return start_logits, end_logits"
      ],
      "metadata": {
        "id": "UXqjOB68nq60"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, sequence_length, d_model, num_heads, hidden, num_layers, drop_prob=0.1, pad_token=0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.pad_token = pad_token\n",
        "    self.d_model = d_model\n",
        "\n",
        "    # Embeddings\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model) # Input Embedding\n",
        "    self.pos_enc = PositionalEncoding(sequence_length, d_model, drop_prob) # Positional Encoding\n",
        "\n",
        "    self.layers = nn.ModuleList([\n",
        "        DecoderBlock(d_model, num_heads, hidden, drop_prob)\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "\n",
        "    self.out = nn.Linear(d_model, vocab_size) # Final Linear Layer for Predicting Probablities\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "\n",
        "    seq_len = input_ids.size(1)\n",
        "\n",
        "    # Creating masks\n",
        "    pad_mask_tensor = pad_mask(input_ids, self.pad_token)\n",
        "    causal = causal_mask(seq_len, input_ids.device)\n",
        "\n",
        "    # Combine masks for self-attention\n",
        "    padding_expanded = pad_mask_tensor.expand(-1, -1, seq_len, -1)\n",
        "    padding_self_attn = padding_expanded & padding_expanded.transpose(-1, -2)\n",
        "    combined_mask = combine_masks(causal, padding_self_attn)\n",
        "\n",
        "    # Embedding + Positional Encoding\n",
        "    y = self.embedding(input_ids) * math.sqrt(self.d_model) # sqrt(d_model) is multiplied to Stabilize Variance as nn.Embedding generate number between 0 and 1\n",
        "    y = self.pos_enc(y) # Applying Positional Encodings\n",
        "\n",
        "    # Passing Through Multiple Decoder Layers (No Cross Attention)\n",
        "    for layer in self.layers:\n",
        "      y = layer(y, encoder_output=None, self_mask=combined_mask)\n",
        "\n",
        "    logits = self.out(y)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "fCXriG3NrG42"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, sequence_length, d_model, num_heads, hidden, num_encoder_layers, num_decoder_layers, drop_prob, pad_token=0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.pad_token = pad_token\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.encoder_embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.decoder_embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    self.encoder_pos_enc = PositionalEncoding(sequence_length, d_model, drop_prob)\n",
        "    self.decoder_pos_enc = PositionalEncoding(sequence_length, d_model, drop_prob)\n",
        "\n",
        "    self.encoder_layers = nn.ModuleList([\n",
        "        EncoderBlock(d_model, num_heads, hidden, drop_prob)\n",
        "        for _ in range(num_encoder_layers)\n",
        "    ])\n",
        "\n",
        "    self.decoder_layers = nn.ModuleList([\n",
        "        DecoderBlock(d_model, num_heads, hidden, drop_prob)\n",
        "        for _ in range(num_decoder_layers)\n",
        "    ])\n",
        "\n",
        "    self.out = nn.Linear(d_model, vocab_size) # Final Linear Layer for Predicting Probablities\n",
        "\n",
        "  def encode(self, src_ids):\n",
        "\n",
        "    # Creating Padding mask\n",
        "    src_pad_mask = pad_mask(src_ids, self.pad_token)\n",
        "\n",
        "    # Embedding + Positional Encoding\n",
        "    x = self.encoder_embedding(src_ids) * math.sqrt(self.d_model) # sqrt(d_model) is multiplied to Stabilize Variance as nn.Embedding generate number between 0 and 1\n",
        "    x = self.encoder_pos_enc(x) # Applying Positional Encodings\n",
        "\n",
        "    # Passing Through Multiple Encoder Layers\n",
        "    for layer in self.encoder_layers:\n",
        "      x = layer(x, src_pad_mask)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def decode(self, tgt_ids, encoder_output, src_ids):\n",
        "\n",
        "    tgt_len = tgt_ids.size(1)\n",
        "\n",
        "    # Decoder self-attention mask (causal + padding)\n",
        "    tgt_pad_mask = pad_mask(tgt_ids, self.pad_token)\n",
        "    causal = causal_mask(tgt_len, tgt_ids.device)\n",
        "\n",
        "    tgt_padding_expanded = tgt_pad_mask.expand(-1, -1, tgt_len, -1)\n",
        "    tgt_padding_self_attn = tgt_padding_expanded & tgt_padding_expanded.transpose(-1, -2)\n",
        "    self_mask = combine_masks(causal, tgt_padding_self_attn)\n",
        "\n",
        "    # Cross Attention Mask\n",
        "    cross_attn_mask = cross_mask(src_ids, tgt_ids, self.pad_token, self.pad_token)\n",
        "\n",
        "    # Embedding + Positional Encoding\n",
        "    y = self.decoder_embedding(tgt_ids) * math.sqrt(self.d_model) # sqrt(d_model) is multiplied to Stabilize Variance as nn.Embedding generate number between 0 and 1\n",
        "    y = self.decoder_pos_enc(y) # Applying Positional Encodings\n",
        "\n",
        "    for layer in self.decoder_layers:\n",
        "      y = layer(y, encoder_output, self_mask=self_mask, cross_mask=cross_attn_mask)\n",
        "\n",
        "    logits = self.out(y)\n",
        "    return logits # [batch_size, tgt_len, vocab_size]\n",
        "\n",
        "  def forward(self, src_ids, tgt_ids):\n",
        "\n",
        "    # Encoder Block\n",
        "    encoder_output = self.encode(src_ids)\n",
        "\n",
        "    # Decoder Block\n",
        "    logits = self.decode(tgt_ids, encoder_output, src_ids)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "dFw4nv3RtsGt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NeuroFormer"
      ],
      "metadata": {
        "id": "Y2_4asj1x7fL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuroFormer(nn.Module):\n",
        "\n",
        "  def __init__(self, mode, vocab_size, sequence_length, d_model, num_heads, hidden, num_layers, drop_prob=0.1, num_encoder_layers=None, num_decoder_layers=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mode = mode\n",
        "\n",
        "    if mode == 'encoder_only':\n",
        "      self.model = Encoder(vocab_size, sequence_length, d_model, num_heads, hidden, num_layers, drop_prob)\n",
        "\n",
        "    elif mode == 'decoder_only':\n",
        "      self.model = Decoder(vocab_size, sequence_length, d_model, num_heads, hidden, num_layers, drop_prob)\n",
        "\n",
        "    elif mode == 'encoder_decoder':\n",
        "      enc_layers = num_encoder_layers or num_layers\n",
        "      dec_layers = num_decoder_layers or num_layers\n",
        "      self.model = EncoderDecoder(vocab_size, sequence_length, d_model, num_heads, hidden, enc_layers, dec_layers, drop_prob)\n",
        "\n",
        "  def forward(self, *args, **kwargs):\n",
        "        return self.model(*args, **kwargs)"
      ],
      "metadata": {
        "id": "rdM6tBqXyEvq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_neuroformer_models():\n",
        "\n",
        "    # Common parameters\n",
        "    vocab_size = 10000\n",
        "    sequence_length = 256\n",
        "    d_model = 256\n",
        "    num_heads = 8\n",
        "    hidden = 1024\n",
        "    num_layers = 6\n",
        "\n",
        "    # 1. Encoder Only - for QA\n",
        "    bert = NeuroFormer(\n",
        "        mode='encoder_only',\n",
        "        vocab_size=vocab_size,\n",
        "        sequence_length=sequence_length,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        hidden=hidden,\n",
        "        num_layers=num_layers\n",
        "    )\n",
        "\n",
        "    # 2. Decoder Only - for language modeling and Chat\n",
        "    gpt = NeuroFormer(\n",
        "        mode='decoder_only',\n",
        "        vocab_size=vocab_size,\n",
        "        sequence_length=sequence_length,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        hidden=hidden,\n",
        "        num_layers=num_layers\n",
        "    )\n",
        "\n",
        "    # 3. Encoder-Decoder - for translation\n",
        "    t5 = NeuroFormer(\n",
        "        mode='encoder_decoder',\n",
        "        vocab_size=vocab_size,\n",
        "        sequence_length=sequence_length,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        hidden=hidden,\n",
        "        num_layers=num_layers,\n",
        "        num_encoder_layers=2,\n",
        "        num_decoder_layers=2\n",
        "    )\n",
        "\n",
        "    return bert, gpt, t5"
      ],
      "metadata": {
        "id": "FzGXSvKz0HpR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    bert_model, gpt_model, t5_model = create_neuroformer_models()\n",
        "\n",
        "    # Example inputs\n",
        "    batch_size = 4\n",
        "    seq_len = 256\n",
        "    vocab_size = 10000\n",
        "\n",
        "    # Random token IDs\n",
        "    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "\n",
        "    print(\"NeuroFormer Models Created Successfully!\")\n",
        "    print(f\"BERT-like parameters: {sum(p.numel() for p in bert_model.parameters()):,}\")\n",
        "    print(f\"GPT-like parameters: {sum(p.numel() for p in gpt_model.parameters()):,}\")\n",
        "    print(f\"T5-like parameters: {sum(p.numel() for p in t5_model.parameters()):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTeXXO6L0Hlz",
        "outputId": "9e85f093-5384-4b55-c42e-f264b92fd28d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuroFormer Models Created Successfully!\n",
            "BERT-like parameters: 7,364,610\n",
            "GPT-like parameters: 11,516,176\n",
            "T5-like parameters: 11,507,472\n"
          ]
        }
      ]
    }
  ]
}